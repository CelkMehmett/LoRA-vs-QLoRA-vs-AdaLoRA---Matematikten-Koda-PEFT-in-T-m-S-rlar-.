# LoRA vs QLoRA vs AdaLoRA â€” Matematikten Koda, PEFT'in TÃ¼m SÄ±rlarÄ±

Bu repo, **Parameter-Efficient Fine-Tuning (PEFT)** tekniklerini â€” **LoRA**, **QLoRA** ve **AdaLoRA** â€” matematiksel temellerinden baÅŸlayarak, pratik Python kod Ã¶rnekleri ile anlatan kapsamlÄ± bir Ã§alÄ±ÅŸmayÄ± iÃ§erir.  
Notebook, hem teorik aÃ§Ä±klamalarÄ± hem de Hugging Face PEFT kÃ¼tÃ¼phanesi ile Ã§alÄ±ÅŸan eÄŸitim kodlarÄ±nÄ± bir arada sunar.

## ğŸ“š Ä°Ã§erik
- **GiriÅŸ: Neden PEFT?** â€” Tam fine-tuningâ€™in maliyeti, PEFT avantajlarÄ±, adapter kavramÄ±.
- **LoRA** â€” DÃ¼ÅŸÃ¼k-rank adaptÃ¶r mantÄ±ÄŸÄ±, matematik formÃ¼lleri, parametre tasarruf hesaplarÄ±, kod Ã¶rnekleri.
- **QLoRA** â€” 4-bit quantization (NF4 + double quant), bellek optimizasyonu, Hugging Face + bitsandbytes ayarlarÄ±.
- **AdaLoRA** â€” Dinamik rank tahsisi, Ã¶nem skorlarÄ± ile kapasite yÃ¶netimi, hiperparametreler.
- **Hangisini Ne Zaman KullanmalÄ±?** â€” Karar tablolarÄ± ve senaryo Ã¶rnekleri.
- **Ä°leri Ä°puÃ§larÄ±** â€” VRAMâ€™e gÃ¶re ayar tarifleri, gradient checkpointing, flash-attention, adapter fusion, merge vs deployment.

## ğŸ›  Kurulum
Bu notebookâ€™u Ã§alÄ±ÅŸtÄ±rmak iÃ§in Python 3.10+ ve aÅŸaÄŸÄ±daki kÃ¼tÃ¼phaneler gereklidir:

```bash
pip install -U transformers datasets peft accelerate bitsandbytes
```

EÄŸer GPU kullanÄ±yorsanÄ±z, CUDA sÃ¼rÃ¼mÃ¼nÃ¼ze uygun **bitsandbytes** versiyonunu yÃ¼klemeyi unutmayÄ±n.

## â–¶ï¸ KullanÄ±m
1. Bu repoâ€™yu klonlayÄ±n:
   ```bash
   git clone https://github.com/kullanici_adi/LoRA-QLoRA-AdaLoRA-PEFT.git
   cd LoRA-QLoRA-AdaLoRA-PEFT
   ```

2. Notebookâ€™u aÃ§Ä±n:
   ```bash
   jupyter notebook "LoRA_vs_QLoRA_vs_AdaLoRA _ Matematikten_Koda,_PEFT'in_TÃ¼m SÄ±rlarÄ±.ipynb"
   ```

3. **Hugging Face Token** ile giriÅŸ yapmanÄ±z gerekiyorsa:
   ```python
   from huggingface_hub import login
   login(token="HF_TOKENINIZ")
   ```

4. Ä°lgili hÃ¼creleri Ã§alÄ±ÅŸtÄ±rarak LoRA, QLoRA veya AdaLoRA eÄŸitim Ã¶rneklerini deneyin.

## ğŸ’¡ Ã–rnek Ã‡Ä±ktÄ±lar
- LoRA ile **orta boy model** Ã¼zerinde hÄ±zlÄ± fine-tuning
- QLoRA ile **7B+ modelleri** tek GPUâ€™da eÄŸitme
- AdaLoRA ile **aynÄ± parametre bÃ¼tÃ§esinde** daha yÃ¼ksek kalite

## ğŸ“„ Lisans
Bu proje MIT lisansÄ± ile lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in `LICENSE` dosyasÄ±na bakÄ±n.
